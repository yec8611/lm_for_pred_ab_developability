{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGBERT_MODEL_NAME = \"Exscientia/IgBERT\"\n",
    "IGBERT_EMBEDDING_DIM = 1024\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "N_SPLITS_CV = 5\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('pnas.1616408114.sd01.xlsx')\n",
    "df2 = pd.read_excel('pnas.1616408114.sd02.xlsx') \n",
    "df3 = pd.read_excel('pnas.1616408114.sd03.xlsx')\n",
    "\n",
    "merged_df = df1.merge(df2, on='Name', how='outer').merge(df3, on='Name', how='outer')\n",
    "             \n",
    "df = merged_df[['VH', 'VL', 'HEK Titer (mg/L)']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_label_val = df[\"HEK Titer (mg/L)\"].min()\n",
    "if min_label_val <= 0:\n",
    "    print(f\"Warning: Minimum label value is {min_label_val}. Adding a small shift before log transform.\")\n",
    "    df[\"HEK Titer (mg/L)\"] = df[\"HEK Titer (mg/L)\"].apply(lambda x: x if x > 0 else 1e-6)\n",
    "\n",
    "df[\"log_label\"] = np.log(df[\"HEK Titer (mg/L)\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[\"scaled_label\"] = scaler.fit_transform(df[[\"log_label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED) \n",
    "print(f\"\\nTrain/Validation set size: {len(train_val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntibodyDatasetIgBERT(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.df['VH'] = self.df['VH'].astype(str)\n",
    "        self.df['VL'] = self.df['VL'].astype(str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        heavy = row[\"VH\"]\n",
    "        light = row[\"VL\"]\n",
    "        label = row[\"scaled_label\"]\n",
    "\n",
    "        heavy_spaced = \" \".join(list(heavy))\n",
    "        light_spaced = \" \".join(list(light))\n",
    "\n",
    "        heavy_inputs = self.tokenizer(\n",
    "            heavy_spaced, truncation=True, max_length=self.max_length,\n",
    "            padding='max_length', return_tensors=\"pt\"\n",
    "        )\n",
    "        light_inputs = self.tokenizer(\n",
    "            light_spaced, truncation=True, max_length=self.max_length,\n",
    "            padding='max_length', return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"heavy_input_ids\": heavy_inputs[\"input_ids\"].squeeze(0),\n",
    "            \"heavy_attention_mask\": heavy_inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"light_input_ids\": light_inputs[\"input_ids\"].squeeze(0),\n",
    "            \"light_attention_mask\": light_inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def collate_fn_igbert(batch):\n",
    "    heavy_ids = torch.stack([item[\"heavy_input_ids\"] for item in batch])\n",
    "    heavy_masks = torch.stack([item[\"heavy_attention_mask\"] for item in batch])\n",
    "    light_ids = torch.stack([item[\"light_input_ids\"] for item in batch])\n",
    "    light_masks = torch.stack([item[\"light_attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    return {\n",
    "        \"heavy_input_ids\": heavy_ids, \"heavy_attention_mask\": heavy_masks,\n",
    "        \"light_input_ids\": light_ids, \"light_attention_mask\": light_masks,\n",
    "        \"label\": labels\n",
    "    }\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention_net = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        att_scores = self.attention_net(token_embeddings).squeeze(-1)\n",
    "        att_scores = att_scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        att_weights = torch.softmax(att_scores, dim=-1).unsqueeze(-1)\n",
    "        pooled = torch.sum(token_embeddings * att_weights, dim=1)\n",
    "        return pooled\n",
    "\n",
    "class IgBERTEmbedder(nn.Module):\n",
    "    def __init__(self, igbert_model, embedding_dim, use_attention_pool=True):\n",
    "        super().__init__()\n",
    "        self.igbert_model = igbert_model\n",
    "        for param in self.igbert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.use_attention_pool = use_attention_pool\n",
    "        if use_attention_pool:\n",
    "            self.pooler = AttentionPooling(embedding_dim)\n",
    "        else:\n",
    "            self.pooler = None\n",
    "        print(f\"IgBERTEmbedder initialized. Using attention pooling: {self.use_attention_pool}\")\n",
    "        print(f\"Expected embedding dimension for pooling: {embedding_dim}\")\n",
    "\n",
    "\n",
    "    def forward(self, heavy_ids, heavy_mask, light_ids, light_mask):\n",
    "        heavy_out = self.igbert_model(input_ids=heavy_ids, attention_mask=heavy_mask)\n",
    "        light_out = self.igbert_model(input_ids=light_ids, attention_mask=light_mask)\n",
    "\n",
    "        heavy_hidden = heavy_out.last_hidden_state\n",
    "        light_hidden = light_out.last_hidden_state\n",
    "\n",
    "        if self.pooler is not None:\n",
    "            heavy_repr = self.pooler(heavy_hidden, heavy_mask)\n",
    "            light_repr = self.pooler(light_hidden, light_mask)\n",
    "        else:\n",
    "            heavy_mask_f = heavy_mask.unsqueeze(-1).float()\n",
    "            heavy_sum = (heavy_hidden * heavy_mask_f).sum(dim=1)\n",
    "            heavy_len = heavy_mask_f.sum(dim=1).clamp(min=1e-9)\n",
    "            heavy_repr = heavy_sum / heavy_len\n",
    "\n",
    "            light_mask_f = light_mask.unsqueeze(-1).float()\n",
    "            light_sum = (light_hidden * light_mask_f).sum(dim=1)\n",
    "            light_len = light_mask_f.sum(dim=1).clamp(min=1e-9)\n",
    "            light_repr = light_sum / light_len\n",
    "\n",
    "        combined_embeddings = torch.cat([heavy_repr, light_repr], dim=1)\n",
    "        return combined_embeddings\n",
    "\n",
    "def extract_igbert_embeddings(embedder_model, data_loader, device):\n",
    "    embedder_model.eval()\n",
    "    all_embeddings_list = []\n",
    "    all_labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Extracting IgBERT embeddings\"):\n",
    "            heavy_ids = batch[\"heavy_input_ids\"].to(device)\n",
    "            heavy_mask = batch[\"heavy_attention_mask\"].to(device)\n",
    "            light_ids = batch[\"light_input_ids\"].to(device)\n",
    "            light_mask = batch[\"light_attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].cpu().numpy()\n",
    "\n",
    "            embeddings = embedder_model(heavy_ids, heavy_mask, light_ids, light_mask)\n",
    "            all_embeddings_list.append(embeddings.cpu().numpy())\n",
    "            all_labels_list.append(labels)\n",
    "\n",
    "    X = np.concatenate(all_embeddings_list, axis=0)\n",
    "    y = np.concatenate(all_labels_list, axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbert_tokenizer = AutoTokenizer.from_pretrained(IGBERT_MODEL_NAME)\n",
    "base_igbert_model = AutoModel.from_pretrained(IGBERT_MODEL_NAME)\n",
    "\n",
    "actual_igbert_hidden_size = base_igbert_model.config.hidden_size\n",
    "if actual_igbert_hidden_size != IGBERT_EMBEDDING_DIM:\n",
    "    print(f\"WARNING: Configured IGBERT_EMBEDDING_DIM ({IGBERT_EMBEDDING_DIM}) \"\n",
    "          f\"does not match loaded model's hidden_size ({actual_igbert_hidden_size}). \"\n",
    "          f\"Using actual_model_hidden_size: {actual_igbert_hidden_size} for embedder.\")\n",
    "    current_igbert_embedding_dim = actual_igbert_hidden_size\n",
    "else:\n",
    "    current_igbert_embedding_dim = IGBERT_EMBEDDING_DIM\n",
    "\n",
    "igbert_embedder = IgBERTEmbedder(\n",
    "    igbert_model=base_igbert_model,\n",
    "    embedding_dim=current_igbert_embedding_dim,\n",
    "    use_attention_pool=True\n",
    ").to(device)\n",
    "\n",
    "train_val_dataset = AntibodyDatasetIgBERT(train_val_df, igbert_tokenizer, max_length=MAX_LENGTH)\n",
    "train_val_loader = DataLoader(train_val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_igbert)\n",
    "\n",
    "if not test_df.empty:\n",
    "    test_dataset = AntibodyDatasetIgBERT(test_df, igbert_tokenizer, max_length=MAX_LENGTH)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_igbert)\n",
    "else:\n",
    "    test_loader = None\n",
    "    print(\"Test set is empty. Skipping final test set evaluation with classical models.\")\n",
    "\n",
    "print(\"\\nExtracting IgBERT embeddings for training/validation set...\")\n",
    "X_train_val_embeddings, y_train_val_labels = extract_igbert_embeddings(igbert_embedder, train_val_loader, device)\n",
    "print(f\"Shape of training/validation embeddings (X): {X_train_val_embeddings.shape}\")\n",
    "print(f\"Shape of training/validation labels (y): {y_train_val_labels.shape}\")\n",
    "\n",
    "X_test_embeddings, y_test_labels = None, None\n",
    "if test_loader:\n",
    "    print(\"\\nExtracting IgBERT embeddings for test set...\")\n",
    "    X_test_embeddings, y_test_labels = extract_igbert_embeddings(igbert_embedder, test_loader, device)\n",
    "    print(f\"Shape of test embeddings (X): {X_test_embeddings.shape}\")\n",
    "    print(f\"Shape of test labels (y): {y_test_labels.shape}\")\n",
    "\n",
    "classical_models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=SEED, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=SEED),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=SEED, verbosity=0, n_jobs=-1),\n",
    "    \"SVR_RBF\": SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Starting {N_SPLITS_CV}-Fold Cross-Validation with Classical Models (using IgBERT Embeddings) ---\")\n",
    "kf = KFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=SEED)\n",
    "cv_results_classical = {model_name: {'mse': [], 'spearman': [], 'pearson': []} for model_name in classical_models.keys()}\n",
    "\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(X_train_val_embeddings)):\n",
    "    print(f\"\\n==== Fold {fold_idx+1}/{N_SPLITS_CV} ====\")\n",
    "    X_train_fold, X_val_fold = X_train_val_embeddings[train_indices], X_train_val_embeddings[val_indices]\n",
    "    y_train_fold, y_val_fold = y_train_val_labels[train_indices], y_train_val_labels[val_indices]\n",
    "\n",
    "    for model_name, model_instance in classical_models.items():\n",
    "        print(f\"  Training {model_name}...\")\n",
    "        model_instance.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        val_preds = model_instance.predict(X_val_fold)\n",
    "        \n",
    "        val_mse = mean_squared_error(y_val_fold, val_preds)\n",
    "        val_spearman, _ = spearmanr(y_val_fold, val_preds) if len(np.unique(y_val_fold)) > 1 and len(np.unique(val_preds)) > 1 else (0.0, 0.0)\n",
    "        val_pearson, _ = pearsonr(y_val_fold, val_preds) if len(np.unique(y_val_fold)) > 1 and len(np.unique(val_preds)) > 1 else (0.0, 0.0)\n",
    "        \n",
    "        cv_results_classical[model_name]['mse'].append(val_mse)\n",
    "        cv_results_classical[model_name]['spearman'].append(val_spearman)\n",
    "        cv_results_classical[model_name]['pearson'].append(val_pearson)\n",
    "        \n",
    "        print(f\"    {model_name} - Val MSE: {val_mse:.4f}, Val Spearman: {val_spearman:.4f}, Val Pearson: {val_pearson:.4f}\")\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary (IgBERT Embeddings + Classical Models) ---\")\n",
    "for model_name, metrics in cv_results_classical.items():\n",
    "    avg_mse = np.mean(metrics['mse'])\n",
    "    std_mse = np.std(metrics['mse'])\n",
    "    avg_spearman = np.mean(metrics['spearman'])\n",
    "    std_spearman = np.std(metrics['spearman'])\n",
    "    avg_pearson = np.mean(metrics['pearson'])\n",
    "    std_pearson = np.std(metrics['pearson'])\n",
    "    print(f\"  {model_name}:\")\n",
    "    print(f\"    Avg Val MSE:       {avg_mse:.4f} +/- {std_mse:.4f}\")\n",
    "    print(f\"    Avg Val Spearman:  {avg_spearman:.4f} +/- {std_spearman:.4f}\")\n",
    "    print(f\"    Avg Val Pearson:   {avg_pearson:.4f} +/- {std_pearson:.4f}\")\n",
    "\n",
    "if X_test_embeddings is not None and y_test_labels is not None:\n",
    "    print(\"\\n--- Final Test Set Evaluation (IgBERT Embeddings + Classical Models) ---\")\n",
    "    print(\"  (Models retrained on the full train_val_embeddings set)\")\n",
    "\n",
    "    for model_name, model_prototype in classical_models.items():\n",
    "        print(f\"  Training {model_name} on full train_val_embeddings set...\")\n",
    "        if model_name == \"RandomForest\": model_instance = RandomForestRegressor(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
    "        elif model_name == \"GradientBoosting\": model_instance = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=SEED)\n",
    "        elif model_name == \"XGBoost\": model_instance = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=SEED, verbosity=0, n_jobs=-1)\n",
    "        elif model_name == \"SVR_RBF\": model_instance = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "        else: model_instance = model_prototype\n",
    "\n",
    "        model_instance.fit(X_train_val_embeddings, y_train_val_labels)\n",
    "        \n",
    "        test_preds_scaled = model_instance.predict(X_test_embeddings)\n",
    "        \n",
    "        test_preds_log = scaler.inverse_transform(test_preds_scaled.reshape(-1, 1)).flatten()\n",
    "        y_test_labels_log = scaler.inverse_transform(y_test_labels.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        test_preds_original = np.exp(test_preds_log)\n",
    "        y_test_labels_original = np.exp(y_test_labels_log)\n",
    "\n",
    "        test_mse_scaled = mean_squared_error(y_test_labels, test_preds_scaled)\n",
    "        test_spearman_scaled, _ = spearmanr(y_test_labels, test_preds_scaled) if len(np.unique(y_test_labels)) > 1 and len(np.unique(test_preds_scaled)) > 1 else (0.0, 0.0)\n",
    "        test_pearson_scaled, _ = pearsonr(y_test_labels, test_preds_scaled) if len(np.unique(y_test_labels)) > 1 and len(np.unique(test_preds_scaled)) > 1 else (0.0, 0.0)\n",
    "\n",
    "        test_mse_original = mean_squared_error(y_test_labels_original, test_preds_original)\n",
    "        test_spearman_original, _ = spearmanr(y_test_labels_original, test_preds_original) if len(np.unique(y_test_labels_original)) > 1 and len(np.unique(test_preds_original)) > 1 else (0.0, 0.0)\n",
    "        test_pearson_original, _ = pearsonr(y_test_labels_original, test_preds_original) if len(np.unique(y_test_labels_original)) > 1 and len(np.unique(test_preds_original)) > 1 else (0.0, 0.0)\n",
    "\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        print(f\"    Test MSE (scaled):       {test_mse_scaled:.4f}\")\n",
    "        print(f\"    Test Spearman (scaled):  {test_spearman_scaled:.4f}\")\n",
    "        print(f\"    Test Pearson (scaled):   {test_pearson_scaled:.4f}\")\n",
    "        print(f\"    Test MSE (original):     {test_mse_original:.4f}\")\n",
    "        print(f\"    Test Spearman (original):{test_spearman_original:.4f}\")\n",
    "        print(f\"    Test Pearson (original): {test_pearson_original:.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.scatterplot(x=y_test_labels_original, y=test_preds_original, alpha=0.6)\n",
    "        min_val = min(y_test_labels_original.min(), test_preds_original.min())\n",
    "        max_val = max(y_test_labels_original.max(), test_preds_original.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "        plt.xlabel(\"True HEK Titer (original scale)\")\n",
    "        plt.ylabel(\"Predicted HEK Titer (original scale)\")\n",
    "        plt.title(f\"{model_name} - Test Set (IgBERT Embeddings)\\nSpearman: {test_spearman_original:.3f}\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nTest set embeddings not available. Skipping final test set evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
